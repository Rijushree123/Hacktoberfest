{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"images/dog.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[106, 169, 153],\n",
       "        [106, 169, 153],\n",
       "        [106, 169, 153],\n",
       "        ...,\n",
       "        [111, 176, 167],\n",
       "        [111, 176, 167],\n",
       "        [111, 176, 167]],\n",
       "\n",
       "       [[106, 169, 153],\n",
       "        [106, 169, 153],\n",
       "        [106, 169, 153],\n",
       "        ...,\n",
       "        [111, 176, 167],\n",
       "        [111, 176, 167],\n",
       "        [111, 176, 167]],\n",
       "\n",
       "       [[106, 169, 153],\n",
       "        [106, 169, 153],\n",
       "        [106, 169, 153],\n",
       "        ...,\n",
       "        [111, 176, 167],\n",
       "        [111, 176, 167],\n",
       "        [111, 176, 167]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 61, 135, 107],\n",
       "        [ 61, 134, 108],\n",
       "        [ 64, 135, 109],\n",
       "        ...,\n",
       "        [ 68, 137, 116],\n",
       "        [ 70, 139, 118],\n",
       "        [ 75, 144, 123]],\n",
       "\n",
       "       [[ 62, 133, 107],\n",
       "        [ 63, 134, 108],\n",
       "        [ 66, 134, 111],\n",
       "        ...,\n",
       "        [ 63, 135, 112],\n",
       "        [ 67, 137, 114],\n",
       "        [ 72, 142, 119]],\n",
       "\n",
       "       [[ 62, 133, 107],\n",
       "        [ 62, 133, 107],\n",
       "        [ 65, 132, 111],\n",
       "        ...,\n",
       "        [ 62, 134, 111],\n",
       "        [ 66, 136, 113],\n",
       "        [ 70, 140, 117]]], dtype=uint8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Image\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(492, 980)\n"
     ]
    }
   ],
   "source": [
    "print(img.shape[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we have our image matrix and we want to get the rotation matrix. To get the rotation matrix, we use the getRotationMatrix2D() method of cv2. The syntax of getRotationMatrix2D() is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__cv2.getRotationMatrix2D(center, angle, scale)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the center is the center point of rotation, the angle is the angle in degrees and scale is the scale property which makes the image fit on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = img.shape[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotationMatrix = cv2.getRotationMatrix2D((width/2,height/2),90,.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotatedImage = cv2.warpAffine(img,rotationMatrix,(width,height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Rotated Image', rotatedImage)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crop an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get the starting and ending index of the row and column. This will define the size of the newly created image. For example, start from row number 10 till row number 15 will give the height of the image.\n",
    "\n",
    "Similarly, start from column number 10 until column number 15 will give the width of the image.\n",
    "\n",
    "You can get the starting point by specifying the percentage value of the total height and the total width. Similarly, to get the ending point of the cropped image, specify the percentage values as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "startRow = int(height*.15)\n",
    "\n",
    "startcol = int(width*.15)\n",
    "\n",
    "endRow = int(height*.85)\n",
    "\n",
    "endcol = int(width*.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now map these values to the original image. Note that you have to cast the starting and ending values to integers because when mapping, the indexes are always integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "croppedImage = img[startRow:endRow, startcol:endcol]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specified the range from starting to ending of rows and columns.\n",
    "\n",
    "Now display the original and cropped image in the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Original Image', img)\n",
    "\n",
    "cv2.imshow('Cropped Image', croppedImage)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resize an Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resize an image, you can use the resize() method of openCV. In the resize method, you can either specify the values of x and y axis or the number of rows and columns which tells the size of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "newImg = cv2.resize(img, (0,0), fx=0.75, fy=0.75)\n",
    "\n",
    "cv2.imshow('Resized Image', newImg)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the row and column values to resize the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "newImg = cv2.resize(img, (550, 350))\n",
    "\n",
    "cv2.imshow('Resized Image', newImg)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust Image Contrast\n",
    "\n",
    "In Python OpenCV module, there is no particular function to adjust image contrast but the official documentation of OpenCV suggests an equation that can perform image brightness and image contrast both at the same time.\n",
    "\n",
    "__new_img = a * original_img + b__\n",
    "\n",
    "Here a is alpha which defines contrast of the image. If a is greater than 1, there will be higher contrast.\n",
    "\n",
    "If the value of a is between 0 and 1 (smaller than 1 but greater than 0), there would be lower contrast. If a is 1, there will be no contrast effect on the image.\n",
    "\n",
    "b stands for beta. The values of b vary from -127 to +127.\n",
    "\n",
    "To implement this equation in Python OpenCV, you can use the addWeighted() method. We use The addWeighted() method as it generates the output in the range of 0 and 255 for a 24-bit color image.\n",
    "\n",
    "The syntax of addWeighted() method is as follows:\n",
    "\n",
    "__cv2.addWeighted(source_img1, alpha1, source_img2, alpha2, beta)__\n",
    "\n",
    "This syntax will blend two images, the first source image (source_img1) with a weight of alpha1 and second source image (source_img2).\n",
    "\n",
    "If you only want to apply contrast in one image, you can add a second image source as zeros using NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_img = cv2.addWeighted(img, 2.5, np.zeros(img.shape, img.dtype), 0, 0)\n",
    "\n",
    "cv2.imshow('Original Image', img)\n",
    "\n",
    "cv2.imshow('Contrast Image', contrast_img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make an image blurry\n",
    "## Gaussian Blur\n",
    "\n",
    "To make an image blurry, you can use the GaussianBlur() method of OpenCV.\n",
    "\n",
    "The GaussianBlur() uses the Gaussian kernel. The height and width of the kernel should be a positive and an odd number.\n",
    "\n",
    "Then you have to specify the X and Y direction that is sigmaX and sigmaY respectively. If only one is specified, both are considered the same.\n",
    "\n",
    "Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur_image = cv2.GaussianBlur(img, (7,7), 0)\n",
    "\n",
    "cv2.imshow('Original Image', img)\n",
    "\n",
    "cv2.imshow('Blur Image', blur_image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median Blur\n",
    "\n",
    "In median blurring, the median of all the pixels of the image is calculated inside the kernel area. The central value is then replaced with the resultant median value. Median blurring is used when there are salt and pepper noise in the image.\n",
    "\n",
    "To apply median blurring, you can use the medianBlur() method of OpenCV.\n",
    "\n",
    "Consider the following example where we have a salt and pepper noise in the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur_image = cv2.medianBlur(img,5)\n",
    "\n",
    "#This will apply 50% noise in the image along with median blur. Now show the images:\n",
    "\n",
    "cv2.imshow('Original Image', img)\n",
    "\n",
    "cv2.imshow('Blur Image', blur_image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Edges\n",
    "\n",
    "To detect the edges in an image, you can use the Canny() method of cv2 which implements the Canny edge detector. The Canny edge detector is also known as the optimal detector.\n",
    "\n",
    "The syntax to Canny() is as follows:\n",
    "\n",
    "__cv2.Canny(image, minVal, maxVal)__\n",
    "\n",
    "Here minVal and maxVal are the minimum and maximum intensity gradient values respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_img = cv2.Canny(img,100,200)\n",
    "\n",
    "cv2.imshow(\"Detected Edges\", edge_img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert image to grayscale (Black & White)\n",
    "\n",
    "The easy way to convert an image in grayscale is to load it like this:\n",
    "\n",
    "__img = cv2.imread(\"pyimg.jpg\", 0)__\n",
    "\n",
    "There is another method using BGR2GRAY.\n",
    "\n",
    "To convert a color image into a grayscale image, use the BGR2GRAY attribute of the cv2 module. This is demonstrated in the example below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cvtColor() method of the cv2 module which takes the original image and the COLOR_BGR2GRAY attribute as an argument. Store the resultant image in a variable:\n",
    "\n",
    "__gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "\n",
    "cv2.imshow(\"Gray Scale Image\", gray_img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroid (Center of blob) detection\n",
    "\n",
    "To find the center of an image, the first step is to convert the original image into grayscale. We can use the cvtColor() method of cv2 as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the image and convert it to a grayscale image. The new image is stored in gray_img.\n",
    "\n",
    "Now we have to calculate the moments of the image. Use the moments() method of cv2. In the moments() method, the grayscale image will be passed as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "moment = cv2.moments(gray_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to calculate the x and y coordinates of the center of the image by using the moments that we got above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = int(moment [\"m10\"] / moment[\"m00\"])\n",
    "\n",
    "Y = int(moment [\"m01\"] / moment[\"m00\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have the center of the image. To highlight this center position, we can use the circle method which will create a circle in the given coordinates of the given radius.\n",
    "\n",
    "The circle() method takes the img, the x and y coordinates where the circle will be created, the size, the color that we want the circle to be and the thickness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[106, 169, 153],\n",
       "        [106, 169, 153],\n",
       "        [106, 169, 153],\n",
       "        ...,\n",
       "        [111, 176, 167],\n",
       "        [111, 176, 167],\n",
       "        [111, 176, 167]],\n",
       "\n",
       "       [[106, 169, 153],\n",
       "        [106, 169, 153],\n",
       "        [106, 169, 153],\n",
       "        ...,\n",
       "        [111, 176, 167],\n",
       "        [111, 176, 167],\n",
       "        [111, 176, 167]],\n",
       "\n",
       "       [[106, 169, 153],\n",
       "        [106, 169, 153],\n",
       "        [106, 169, 153],\n",
       "        ...,\n",
       "        [111, 176, 167],\n",
       "        [111, 176, 167],\n",
       "        [111, 176, 167]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 61, 135, 107],\n",
       "        [ 61, 134, 108],\n",
       "        [ 64, 135, 109],\n",
       "        ...,\n",
       "        [ 68, 137, 116],\n",
       "        [ 70, 139, 118],\n",
       "        [ 75, 144, 123]],\n",
       "\n",
       "       [[ 62, 133, 107],\n",
       "        [ 63, 134, 108],\n",
       "        [ 66, 134, 111],\n",
       "        ...,\n",
       "        [ 63, 135, 112],\n",
       "        [ 67, 137, 114],\n",
       "        [ 72, 142, 119]],\n",
       "\n",
       "       [[ 62, 133, 107],\n",
       "        [ 62, 133, 107],\n",
       "        [ 65, 132, 111],\n",
       "        ...,\n",
       "        [ 62, 134, 111],\n",
       "        [ 66, 136, 113],\n",
       "        [ 70, 140, 117]]], dtype=uint8)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.circle(img, (X, Y), 15, (205, 114, 101), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Center of the Image\", img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply a mask for a colored image\n",
    "\n",
    "Image masking means to apply some other image as a mask on the original image or to change the pixel values in the image.\n",
    "\n",
    "To apply a mask on the image, we will use the HoughCircles() method of the OpenCV module. The HoughCircles() method detects the circles in an image. After detecting the circles, we can simply apply a mask on these circles.\n",
    "\n",
    "The HoughCircles() method takes the original image, the Hough Gradient (which detects the gradient information in the edges of the circle), and the information from the following circle equation:\n",
    "\n",
    "(x - xcenter)2 + (y - ycenter)2 = r2 \n",
    "\n",
    "In this equation (xcenter , ycenter) is the center of the circle and r is the radius of the circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting the circles in the image using the HoughCircles() code from OpenCV: __Hough Circle Transform:__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_img = cv2.medianBlur(cv2.cvtColor(img, cv2.COLOR_RGB2GRAY), 3)\n",
    "\n",
    "circles = cv2.HoughCircles(gray_img, cv2.HOUGH_GRADIENT, 1, 20, param1=50, param2=50, minRadius=0, maxRadius=0)\n",
    "\n",
    "circles = np.uint16(np.around(circles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "masking=np.full((img1.shape[0], img1.shape[1]),0,dtype=np.uint8)\n",
    "\n",
    "for j in circles[0, :]:\n",
    "\n",
    "    cv2.circle(masking, (j[0], j[1]), j[2], (255, 255, 255), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_img = cv2.bitwise_or(img1, img1, mask=masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('FinalImage',final_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce Noise\n",
    "\n",
    "To reduce noise from an image, OpenCV provides the following methods:\n",
    "\n",
    "    fastNlMeansDenoising(): Removes noise from a grayscale image\n",
    "    fastNlMeansDenoisingColored(): Removes noise from a colored image\n",
    "    fastNlMeansDenoisingMulti(): Removes noise from grayscale image frames (a grayscale video)\n",
    "    fastNlMeansDenoisingColoredMulti(): Same as 3 but works with colored frames\n",
    "\n",
    "Let’s use fastNlMeansDenoisingColored() in our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img = cv2.imread(\"./images/dog.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the denoising function which takes respectively the original image (src), the destination (which we have kept none as we are storing the resultant), the filter strength, the image value to remove the colored noise (usually equal to filter strength or 10), the template patch size in pixel to compute weights which should always be odd (recommended size equals 7) and the window size in pixels to compute average of the given pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cv2.fastNlMeansDenoisingColored(img,None,20,10,7,21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Original Image\", img)\n",
    "\n",
    "cv2.imshow(\"Denoised Image\", result)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get image contour\n",
    "\n",
    "Contours are the curves in an image that are joint together. The curves join the continuous points in an image. The purpose of contours is used to detect the objects.\n",
    "\n",
    "The original image of which we are getting the contours of is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread('images/dog.jpg')\n",
    "\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "retval, thresh = cv2.threshold(gray_img, 127, 255, 0)\n",
    "img_contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cv2.drawContours(img, img_contours, -1, (0, 255, 0))\n",
    "cv2.imshow('Image Contours', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Images into Cartoon using OpenCV\n",
    "To convert an image into cartoon, we need to detect the edges of the object in the image. Then, we're changing the effects of the image by using\n",
    "Gaussian Blur to reduce the noise and then applying a bilateral filter effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "img = cv2.imread(\"images/emma.png\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "gray = cv2.GaussianBlur(gray,(5,5),-1)\n",
    "edges = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY,9,10)\n",
    "color = cv2.bilateralFilter(img, 20, 245, 245)\n",
    "cartoon = cv2.bitwise_and(color, color, mask=edges)\n",
    "cv2.imshow(\"Cartoon\",cartoon)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Background from an image\n",
    "\n",
    "To remove the background from an image, we will find the contours to detect edges of the main object and create a mask with np.zeros for the background and then combine the mask and the image using the bitwise_and operator.\n",
    "\n",
    "Consider the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"images/emma.png\")\n",
    "\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "_, thresh = cv2.threshold(gray_img, 127, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "img_contours = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "img_contours = sorted(img_contours, key=cv2.contourArea)\n",
    "\n",
    "for i in img_contours:\n",
    "\n",
    "    if cv2.contourArea(i) > 100:\n",
    "\n",
    "        break\n",
    "        \n",
    "mask = np.zeros(img.shape[:2], np.uint8)\n",
    "cv2.drawContours(mask, [i],-1, 255, -1)\n",
    "new_img = cv2.bitwise_and(img, img, mask=mask)\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "cv2.imshow(\"Image with background removed\", new_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
